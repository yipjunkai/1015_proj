{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of outwardly depressive mood on social media\n",
    "\n",
    "Use of Natural Language Processing on posts made on Twitter and Reddit to predict depressive thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook is written to be run both locally or on Google Colab.\n",
    "\n",
    "## Setup for local run\n",
    "\n",
    "- Download the root file as is.\n",
    "- Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install nltk\n",
    "# ! pip install pickle\n",
    "# ! pip install keras\n",
    "# ! pip install tqdm\n",
    "# ! pip install dask\n",
    "# ! pip install seaborn\n",
    "# ! pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab\n",
    "\n",
    "- Download this notebook and upload onto Google Colab\n",
    "- Download the zip files (within /input) and upload into root directory of your Google Drive.\n",
    "\n",
    "*You may download the [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) directly from the source and replace the provided one (within /input). No edits were made to the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from re import sub\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLocally():\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"Running locally...\")\n",
    "\n",
    "    path = './build'\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        for x in os.listdir('./input'):\n",
    "            shutil.unpack_archive(f'./input/{x}', path)\n",
    "            print(f\"Extracted {x} into '{path}' directory\")\n",
    "    else:\n",
    "        print(f\"{path} directory already exists. Skipping extracting of zip files.\")\n",
    "\n",
    "    gpuCount = len(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    if gpuCount > 0:\n",
    "\n",
    "        print(f\"{gpuCount} GPUs detected.\")\n",
    "\n",
    "        if tf.test.is_built_with_cuda():\n",
    "            print(f\"Tensorflow has CUDA support.\")\n",
    "\n",
    "        if not tf.test.is_built_with_cuda():\n",
    "            print(\"Tensorflow doesn't have CUDA support.\")\n",
    "    else:\n",
    "        print(\"No GPUs detected on local device.\")\n",
    "\n",
    "    return path\n",
    "\n",
    "def runOnColab():\n",
    "\n",
    "    from google.colab import drive\n",
    "    \n",
    "    print(\"Running on Google Colab\")\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !unzip \"/content/drive/MyDrive/training.1600000.processed.noemoticon.csv.zip\"\n",
    "    !unzip \"/content/drive/MyDrive/scrapped_posts.zip\"\n",
    "\n",
    "    return '/content'\n",
    "    \n",
    "directory = runLocally()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model from pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(pathToModel, pathToPKL):\n",
    "    with open(pathToPKL, 'rb') as f:\n",
    "        tokeniser = pickle.load(f)\n",
    "    return load_model(pathToModel), tokeniser\n",
    "\n",
    "model, tokeniser = loadModel(\"./model.h5\", \"./tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeInvalidRedditPost(df):\n",
    "    df = df[df.Body.notna()]\n",
    "    df = df[df.Author != \"[removed]\"]\n",
    "    df = df[df.Body != \"[removed]\"]\n",
    "    df = df[df.Author != \"[deleted]\"]\n",
    "    df = df[df.Body != \"[deleted]\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def standardiseRedditDF(dff, sentimentValue=None):\n",
    "\n",
    "    dff = removeInvalidRedditPost(dff)\n",
    "    dff.rename(columns={'Author': 'user_id', 'Post_iD': 'id', 'Publish_date':'date', 'Body':'text'}, inplace=True)\n",
    "    dff['text'] = dff['Title'].str.cat(dff['text'], sep=\" \")\n",
    "    dff = dff.drop(columns=['Score', 'Total_no_of_comments', 'Link', 'Subreddit', 'Title'])\n",
    "    \n",
    "    if sentimentValue != None:\n",
    "        dff['sentiment'] = sentimentValue\n",
    "\n",
    "    return dff\n",
    "\n",
    "def preprocess(text):\n",
    "  \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = \"@[^\\s]+\" \n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "\n",
    "    text = sub(urlPattern, ' URL', text).strip()\n",
    "    text = sub(userPattern, ' USER', text).strip()\n",
    "    text = sub(alphaPattern, ' ', text).strip()\n",
    "    \n",
    "    stopWords = stopwords.words(\"english\")\n",
    "    tokens = list(filter(lambda x: x not in stopWords, text.split()))\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def parellelPreProcess(df):\n",
    "    ddf = dd.from_pandas(df, npartitions=4)\n",
    "    ddf[\"text\"] = ddf[\"text\"].map(lambda x: preprocess(x), meta=('result', str))\n",
    "    return ddf.compute()\n",
    "\n",
    "def sentimentFromScore(score):\n",
    "  score = float(score)\n",
    "  label = 'Neutral'\n",
    "  if score <= 0.35:\n",
    "      label = 'Negative'\n",
    "  elif score >= 0.65:\n",
    "      label = 'Positive'\n",
    "\n",
    "  return label\n",
    "\n",
    "def predict(text, wantsTime=False):\n",
    "  if wantsTime: \n",
    "      start_at = time()\n",
    "  \n",
    "  text = str(text)\n",
    "  score = model.predict(pad_sequences(tokeniser.texts_to_sequences([text]), maxlen=300))\n",
    "\n",
    "  result = {\"label\": sentimentFromScore(score), \n",
    "            \"score\": score}\n",
    "  \n",
    "  if wantsTime: \n",
    "      result[\"elapsedTime\"] = time() - start_at\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on uncategorised subreddits\n",
    "\n",
    "## Preparing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubreddit(file, count = 0):\n",
    "    df4 = pd.read_csv(f\"{directory}/{file}\")\n",
    "    df4 = standardiseRedditDF(df4)\n",
    "    df4 = parellelPreProcess(df4)\n",
    "\n",
    "    if count > 0:\n",
    "        if count <= df4.index.count():\n",
    "            df4 = df4.head(count)\n",
    "        \n",
    "    ddf = dd.from_pandas(df4, npartitions=4)\n",
    "    ddf[\"sentimentScore\"] = ddf[\"text\"].map(lambda x: float(predict(x)['score']), meta=('result', float))\n",
    "    ddf[\"sentiment\"] = ddf[\"sentimentScore\"].map(lambda x: sentimentFromScore(x), meta=('result', str))\n",
    "    df4 = ddf.compute()\n",
    "    \n",
    "    return df4\n",
    "\n",
    "teenagersDF = processSubreddit('teenagers.csv', 4000)\n",
    "teenagersDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequentPosterDF(df):\n",
    "    \n",
    "    resultDF = df.groupby('user_id').filter(lambda x : x['user_id'].shape[0]>=3)\n",
    "\n",
    "    return resultDF\n",
    "\n",
    "def changeOfSentimentOverTime(df, subset = 0):\n",
    "\n",
    "    significantDiff = 0.4\n",
    "    \n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axs[0].set_title(f\"Positive change\")\n",
    "    axs[0].legend().set_visible(False)\n",
    "    axs[1].set_title(f\"Negative change\")\n",
    "    axs[1].legend().set_visible(False)\n",
    "\n",
    "    df = frequentPosterDF(df)\n",
    "    names = df.user_id.unique()\n",
    "    \n",
    "    if subset > 0:\n",
    "        if subset <= len(names):\n",
    "            random.shuffle(names)\n",
    "            names = names[0:subset]\n",
    "    \n",
    "    for name in names:\n",
    "        singleUserDF = df[df[\"user_id\"] == name].sort_values(\"date\")\n",
    "\n",
    "        diff = singleUserDF['sentimentScore'].iat[-1] - singleUserDF['sentimentScore'].iat[0]\n",
    "\n",
    "        if diff >= significantDiff:\n",
    "            singleUserDF.plot(x='date', y='sentimentScore', kind='line', figsize=(20,8), ax=axs[0])\n",
    "        elif diff <= -significantDiff:\n",
    "            singleUserDF.plot(x='date', y='sentimentScore', kind='line', figsize=(20,8), ax=axs[1])\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "changeOfSentimentOverTime(teenagersDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgSentimentScoreOverPostingFreq(df, subset=0):\n",
    "    \n",
    "    fig, axs = plt.subplots(1)\n",
    "    fig.tight_layout()\n",
    "    axs.set_title(f\"Avg sentimentScore against No. of Posts of user\")\n",
    "    axs.legend().set_visible(False)\n",
    "    \n",
    "    names = df.user_id.unique()\n",
    "\n",
    "    if subset > 0:\n",
    "        if subset <= len(names):\n",
    "            random.shuffle(names)\n",
    "            names = names[0:subset]\n",
    "    \n",
    "    tempdf = pd.DataFrame()\n",
    "    tempdf['user_id'] = pd.DataFrame(names)\n",
    "    \n",
    "    tempListMean = []\n",
    "    tempListCount = []\n",
    "\n",
    "    for name in names:\n",
    "        tempListMean.append(df[df['user_id'] == name]['sentimentScore'].mean())\n",
    "        tempListCount.append(len(df[df['user_id'] == name].index))\n",
    "    \n",
    "    tempdf['avgSentimentScore'] = pd.DataFrame(tempListMean)\n",
    "    tempdf['postFreq'] = pd.DataFrame(tempListCount)\n",
    "    \n",
    "    tempdf.plot(x='postFreq', y='avgSentimentScore', kind='hist', figsize=(20,8), ax=axs)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "avgSentimentScoreOverPostingFreq(teenagersDF)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
