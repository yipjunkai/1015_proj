{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis of outwardly depressive modd on social media\n",
        "\n",
        "Use of Natural Language Processing on posts made on Twitter and Reddit to predict depressive thoughts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This notebook is written to be run both locally or on Google Colab.\n",
        "\n",
        "## Setup for local run\n",
        "\n",
        "- Download the root file as is.\n",
        "- Install packages\n",
        "\n",
        "```python\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "\n",
        "## Setup for Google Colab\n",
        "\n",
        "- Download this notebook and upload onto Google Colab\n",
        "- Download the zip files (within /input) and upload into root directory of your Google Drive.\n",
        "\n",
        "*You may download the [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) directly from the source and replace the provided one (within /input). No edits were made to the data.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5PTnw_b3n0s",
        "outputId": "c9e21091-a13c-4646-ef86-873c9141185e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from re import sub\n",
        "from time import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from matplotlib.pyplot import show\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def runLocally():\n",
        "\n",
        "    import shutil\n",
        "    import os\n",
        "    import tensorflow as tf\n",
        "\n",
        "    print(\"Running locally...\")\n",
        "\n",
        "    path = './build'\n",
        "\n",
        "    if not os.path.isdir(path):\n",
        "        for x in os.listdir('./input'):\n",
        "            shutil.unpack_archive(f'./input/{x}', path)\n",
        "            print(f\"Extracted {x} into '{path}' directory\")\n",
        "    else:\n",
        "        print(f\"{path} directory already exists. Skipping extracting of zip files.\")\n",
        "\n",
        "    gpuCount = len(tf.config.list_physical_devices('GPU'))\n",
        "    \n",
        "    if gpuCount > 0:\n",
        "\n",
        "        print(f\"{gpuCount} GPUs detected.\")\n",
        "\n",
        "        if tf.test.is_built_with_cuda():\n",
        "            print(f\"Tensorflow has CUDA support.\")\n",
        "\n",
        "        if not tf.test.is_built_with_cuda():\n",
        "            print(\"Tensorflow doesn't have CUDA support.\")\n",
        "    else:\n",
        "        print(\"No GPUs detected on local device.\")\n",
        "\n",
        "    return path\n",
        "\n",
        "def runOnColab():\n",
        "\n",
        "    from google.colab import drive\n",
        "    \n",
        "    print(\"Running on Google Colab\")\n",
        "    \n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    !unzip \"/content/drive/MyDrive/training.1600000.processed.noemoticon.csv.zip\"\n",
        "    !unzip \"/content/drive/MyDrive/scrapped_posts.zip\"\n",
        "\n",
        "    return '/content'\n",
        "    \n",
        "directory = runLocally()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7D8CkiY3aFY"
      },
      "source": [
        "# **Loading data**\n",
        "\n",
        "We are using 2 sources of data, pre-catagorised twitter posts from kaggle, and scrapped reddit post from specific subreddits.\n",
        "\n",
        "1. Twitter Posts from Kaggle\n",
        "\n",
        "2. Scrape posts from subreddits; [/r/depression](https://www.reddit.com/r/depression/), [/r/suicidewatch](https://www.reddit.com/r/SuicideWatch/)\n",
        "\n",
        "This is to allow for a greater vocabulary between the two different websites for more general NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnlQ2T5c0yXV"
      },
      "source": [
        "---\n",
        "\n",
        "## Twitter data from [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "OJBdnkVJ4ccm",
        "outputId": "cf26fdb7-f64e-4e0a-a986-1bdf3b4e7e98"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(f'{directory}/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)\n",
        "df1.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bB6VzG6cCLQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Reddit data from scrapper\n",
        "\n",
        "*Below is a code block of the scrapping code we ran ahead of time. The raw data was exported to csv files and zipped into /input/scrapped_posts.zip as it tooks hours to scrapped the data and there were limitations with the api used.*\n",
        "\n",
        "*Note: Since last touched, the api endpoints may have changed.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtMze7GGcjUp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "QrkWhLc4eECq",
        "outputId": "94e2d906-f8ac-4d51-fc5a-ac354d653e20"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(f\"{directory}/depression.csv\")\n",
        "df3 = pd.read_csv(f\"{directory}/suicide_watch.csv\")\n",
        "\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnDlcA7Blk4a"
      },
      "source": [
        "---\n",
        "\n",
        "# Merging data twitter-reddit\n",
        "\n",
        "- Standardise twitter columns\n",
        "  - Drop excess columns\n",
        "- Standardise reddit columns to twitter columns\n",
        "  - Rename columns\n",
        "  - Merge title and body (reddit posts) into body ~ twitter posts don't have titles\n",
        "  - Drop excess columns\n",
        "- Assign sentiment score based on subreddit pulled from\n",
        "- Merge into single dataframe\n",
        "\n",
        "*Assume posts from same subreddit have similar sentiment score; posts from [/r/depression](https://www.reddit.com/r/depression/), [/r/suicidewatch](https://www.reddit.com/r/SuicideWatch/) are negative.*\n",
        "\n",
        "*The size of twitter data from [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) far exceeds that of reddit data, so this assumption does not effect the data much.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NRkomAyHsn-s",
        "outputId": "6f329e31-367c-4010-ea5e-fa4397deed64"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop(columns=['query'])\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "89TL01NjhuMs",
        "outputId": "945c2a84-67de-4fee-d23c-ec9c71e28087"
      },
      "outputs": [],
      "source": [
        "def standardiseRedditDF(dff, sentimentValue=None):\n",
        "\n",
        "    dff.rename(columns={'Author': 'user_id', 'Post_iD': 'id', 'Publish_date':'date', 'Body':'text'}, inplace=True)\n",
        "    dff['text'] = dff['Title'].str.cat(dff['text'], sep=\" - \")\n",
        "    dff = dff.drop(columns=['Score', 'Total_no_of_comments', 'Link', 'Subreddit', 'Title'])\n",
        "    \n",
        "    if sentimentValue != None:\n",
        "        dff['sentiment'] = sentimentValue\n",
        "\n",
        "    return dff\n",
        "\n",
        "df2 = standardiseRedditDF(df2, 0)\n",
        "df3 = standardiseRedditDF(df3, 0)\n",
        "\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "A-vfLB6Llo8D",
        "outputId": "c052466e-6671-4d6e-e99a-a40813b0f025"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df1, df2, df3])\n",
        "\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K65cIsYMsjg_"
      },
      "source": [
        "# Prepping data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memXOiMGpIZ3"
      },
      "source": [
        "## Removing invalid posts\n",
        "\n",
        "Reddit posts contains deleted users and removed posts. We need to remove these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC568Sh9pPwf",
        "outputId": "2972d615-c3c1-4d53-e15d-fd4e08c33d38"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def removeInvalidRedditPost(df):\n",
        "    df = df[df.text != \"[removed]\"]\n",
        "    df = df[df.user_id != \"[deleted]\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "df = removeInvalidRedditPost(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp8Ntbz7UK7a"
      },
      "source": [
        "## Mapping sentiments\n",
        "\n",
        "* 0 - negative\n",
        "* 2 - neutral\n",
        "* 4 - positive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhqboZgsUQs6",
        "outputId": "1205556e-a910-4794-a9c9-8d1a3dcf621a"
      },
      "outputs": [],
      "source": [
        "def sentimentMapping(label):\n",
        "    decodeMap = {0: \"Negative\", 2: \"Neutral\", 4: \"Positive\"}\n",
        "    return decodeMap[int(label)]\n",
        "\n",
        "df.sentiment = df.sentiment.progress_apply(lambda x: sentimentMapping(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFq-yKEn6fuZ"
      },
      "source": [
        "## Cleaning text\n",
        "\n",
        "1. Lower casing\n",
        "2. Replacing URLs\n",
        "3. Replacing username references \n",
        "4. Removing non-alphanumerics\n",
        "5. Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQdcjKC26gQ_",
        "outputId": "39bbcb40-2299-4f0d-a0fc-27be6ad8eaa9"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  \n",
        "  stopWords = stopwords.words(\"english\")\n",
        "  urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "  userPattern       = \"@[^\\s]+\" \n",
        "  alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "\n",
        "  text = str(text).lower()\n",
        "  text = sub(urlPattern, ' URL', text).strip()\n",
        "  text = sub(userPattern, ' USER', text).strip()\n",
        "  text = sub(alphaPattern, ' ', text).strip()\n",
        "  \n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stopWords:\n",
        "      tokens.append(token)\n",
        "\n",
        "  return \" \".join(tokens)\n",
        "\n",
        "df.text = df.text.progress_apply(lambda x: preprocess(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaned and merged data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "MYjcKvPmOWx4",
        "outputId": "2325a73e-d30a-42a1-92c5-d812b01f075d"
      },
      "outputs": [],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHJzy58BYuo"
      },
      "source": [
        "## Train and test splitting\n",
        "\n",
        "80/20 split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjJQVZh_BZrq",
        "outputId": "d70cd714-a14f-4eb4-a780-4fc14a283cd0"
      },
      "outputs": [],
      "source": [
        "trainData, testData = train_test_split(df, train_size=0.8)\n",
        "\n",
        "print(\"Train size:\", len(trainData))\n",
        "print(\"Test size:\", len(testData))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1P8ObAqzoAJ"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCFENK6-zu-8",
        "outputId": "6dc4180c-6381-44d7-ace3-c7b7f9183fe4"
      },
      "outputs": [],
      "source": [
        "tokeniser = Tokenizer()\n",
        "tokeniser.fit_on_texts(trainData.text)\n",
        "\n",
        "vocabSize = len(tokeniser.word_index) + 1\n",
        "print(f'Vocab size: {vocabSize}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7z3dlBM1O5L"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWhPyLQq1SS2",
        "outputId": "071ccddd-2423-4909-f315-e5af7db91aa5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(trainData.sentiment.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6n0sT1W1-y4"
      },
      "source": [
        "## Reshaping train and test variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14ICIuBA1oJ_",
        "outputId": "984f658a-00ae-432f-e9d2-813e6b16cec8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "xTrain = pad_sequences(tokeniser.texts_to_sequences(trainData.text), maxlen = 300)\n",
        "xTest = pad_sequences(tokeniser.texts_to_sequences(testData.text), maxlen = 300)\n",
        "\n",
        "yTrain = encoder.transform(trainData.sentiment.to_list()).reshape(-1,1)\n",
        "yTest = encoder.transform(testData.sentiment.to_list()).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wIIokKM2G1L"
      },
      "source": [
        "## Model build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd5EMQWf2WBp",
        "outputId": "bce5e104-7a9e-4837-9ad2-5cdc63a47e2e"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "w2vModel = Word2Vec(vector_size=300, window=7, min_count=10, workers=6)\n",
        "\n",
        "_words = [_text.split() for _text in trainData.text]\n",
        "\n",
        "w2vModel.build_vocab(_words)\n",
        "w2vModel.train(_words, total_examples=len(_words), epochs=8)\n",
        "\n",
        "embMatrix = np.zeros((vocabSize, 300))\n",
        "for word, i in tokeniser.word_index.items():\n",
        "  if word in w2vModel.wv:\n",
        "    embMatrix[i] = w2vModel.wv[word]\n",
        "\n",
        "embLayer = Embedding(vocabSize, 300, weights=[embMatrix], input_length=300, trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK3-FmVK-Ndd",
        "outputId": "154c9202-d44f-48e3-924c-2bff260b6c16"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(embLayer)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Cr-54FFvUh",
        "outputId": "e2157bb2-9929-4baf-97b5-817f176e98f7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0), \n",
        "              EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k6vR2zIGKqR",
        "outputId": "0359efb2-c853-4c51-aa1e-bd498c1015ae"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "history = model.fit(xTrain, yTrain,\n",
        "                    batch_size=1024,\n",
        "                    epochs=8,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving and loading model\n",
        "\n",
        "This function is to save a trained model and load a pre-trained model from model.h5. To use place 'model.h5' file within\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pc_aEQH0596"
      },
      "outputs": [],
      "source": [
        "def saveModel():\n",
        "    model.save(\"model.h5\")\n",
        "    pickle.dump(tokeniser, open(\"tokenizer.pkl\", \"wb\"), protocol=0)\n",
        "\n",
        "def loadModel(pathToModel, pathToPKL):\n",
        "    with open(pathToPKL, 'rb') as f:\n",
        "        tokeniser = pickle.load(f)\n",
        "    return load_model(pathToModel), tokeniser\n",
        "\n",
        "# Example usage\n",
        "# model, tokeniser = loadModel(\"./model.h5\", \"./tokenizer.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qACVjp1BO0TM"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqdBib_zHHc7"
      },
      "outputs": [],
      "source": [
        "def sentimentFromScore(score):\n",
        "  label = 'Neutral'\n",
        "  if score <= 0.35:\n",
        "      label = 'Negative'\n",
        "  elif score >= 0.65:\n",
        "      label = 'Positive'\n",
        "\n",
        "  return label\n",
        "\n",
        "def predict(text):\n",
        "  start_at = time()\n",
        "  score = model.predict(pad_sequences(tokeniser.texts_to_sequences([text]), maxlen=300))\n",
        "\n",
        "  return {\"label\": sentimentFromScore(score), \n",
        "          \"score\": score,\n",
        "          \"elapsedTime\": time() - start_at}\n",
        "\n",
        "prediction = predict(\"I'm sick of this game\")\n",
        "\n",
        "print(f\"Label: {prediction['label']}\")\n",
        "print(f\"Score: {prediction['score']}\")\n",
        "print(f\"Time elapsed: {prediction['elapsedTime']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data from uncategorised subreddits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywovfMkWOmM3"
      },
      "outputs": [],
      "source": [
        "def hi():\n",
        "    df4 = pd.read_csv(f\"{directory}/teenagers.csv\")\n",
        "    df4 = standardiseRedditDF(df4)\n",
        "    df4 = removeInvalidRedditPost(df4)\n",
        "    df4.sentiment = df4.text.progress_apply(lambda x: predict(str(x))['label'])\n",
        "\n",
        "    df4.hist(column=\"date\")\n",
        "    show()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
