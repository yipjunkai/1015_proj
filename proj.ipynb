{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of outwardly depressive mood on social media\n",
    "\n",
    "Use of Natural Language Processing on posts made on Twitter and Reddit to predict depressive thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook is written to be run both locally or on Google Colab.\n",
    "\n",
    "## Setup for local run\n",
    "\n",
    "- Download the root file as is.\n",
    "- Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install numpy\n",
    "# ! pip install nltk\n",
    "# ! pip install pickle\n",
    "# ! pip install keras\n",
    "# ! pip install tqdm\n",
    "# ! pip install dask\n",
    "# ! pip install seaborn\n",
    "# ! pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Google Colab\n",
    "\n",
    "- Download this notebook and upload onto Google Colab\n",
    "- Download the zip files (within /input) and upload into root directory of your Google Drive.\n",
    "\n",
    "*You may download the [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) directly from the source and replace the provided one (within /input). No edits were made to the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5PTnw_b3n0s",
    "outputId": "c9e21091-a13c-4646-ef86-873c9141185e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from re import sub\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, Dropout, CuDNNLSTM, Dense\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "tqdm.pandas()\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.dataframe as dd\n",
    "ProgressBar().register()\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLocally():\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    print(\"Running locally...\")\n",
    "\n",
    "    path = './build'\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        for x in os.listdir('./input'):\n",
    "            shutil.unpack_archive(f'./input/{x}', path)\n",
    "            print(f\"Extracted {x} into '{path}' directory\")\n",
    "    else:\n",
    "        print(f\"{path} directory already exists. Skipping extracting of zip files.\")\n",
    "\n",
    "    gpuCount = len(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    if gpuCount > 0:\n",
    "\n",
    "        print(f\"{gpuCount} GPUs detected.\")\n",
    "\n",
    "        if tf.test.is_built_with_cuda():\n",
    "            print(f\"Tensorflow has CUDA support.\")\n",
    "\n",
    "        if not tf.test.is_built_with_cuda():\n",
    "            print(\"Tensorflow doesn't have CUDA support.\")\n",
    "    else:\n",
    "        print(\"No GPUs detected on local device.\")\n",
    "\n",
    "    return path\n",
    "\n",
    "def runOnColab():\n",
    "\n",
    "    from google.colab import drive\n",
    "    \n",
    "    print(\"Running on Google Colab\")\n",
    "    \n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !unzip \"/content/drive/MyDrive/training.1600000.processed.noemoticon.csv.zip\"\n",
    "    !unzip \"/content/drive/MyDrive/scrapped_posts.zip\"\n",
    "\n",
    "    return '/content'\n",
    "    \n",
    "directory = runLocally()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7D8CkiY3aFY"
   },
   "source": [
    "# **Loading data**\n",
    "\n",
    "We are using 2 sources of data, pre-catagorised twitter posts from kaggle, and scrapped reddit post from specific subreddits.\n",
    "\n",
    "1. Twitter Posts from Kaggle\n",
    "\n",
    "2. Scrape posts from subreddits; [/r/depression](https://www.reddit.com/r/depression/), [/r/suicidewatch](https://www.reddit.com/r/SuicideWatch/)\n",
    "\n",
    "This is to allow for a greater vocabulary between the two different websites for more general NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnlQ2T5c0yXV"
   },
   "source": [
    "---\n",
    "\n",
    "## Twitter data from [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OJBdnkVJ4ccm",
    "outputId": "cf26fdb7-f64e-4e0a-a986-1bdf3b4e7e98"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f'{directory}/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)\n",
    "df1.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bB6VzG6cCLQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Reddit data from scrapper\n",
    "\n",
    "*Below is a code block of the scrapping code we ran ahead of time. The raw data was exported to csv files and zipped into /input/scrapped_posts.zip as it tooks hours to scrapped the data and there were limitations with the api used.*\n",
    "\n",
    "*Note: Since last touched, the api endpoints may have changed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtMze7GGcjUp"
   },
   "outputs": [],
   "source": [
    "def redditScrapper():\n",
    "    from psaw import PushshiftAPI\n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    api_request_generator = api.search_submissions(after=1645459200 ,before=1647878400,filter=['id', 'title', 'author', 'selftext', 'score', 'num_comments', 'created', 'subreddit', 'full_link'],subreddit='depression')\n",
    "\n",
    "    finalframe = pd.DataFrame([submission.d_ for submission in api_request_generator])\n",
    "\n",
    "    finalframe['created'] = pd.to_datetime(finalframe['created'],  unit='s') \n",
    "    finalframe.columns=[\"Post_iD\",\"Title\",\"Author\",\"Body\",\"Score\",\"Total_no_of_comments\", \"Publish_date\",\"Subreddit\",\"Link\"]\n",
    "    \n",
    "    return finalframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "QrkWhLc4eECq",
    "outputId": "94e2d906-f8ac-4d51-fc5a-ac354d653e20"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(f\"{directory}/depression.csv\")\n",
    "df3 = pd.read_csv(f\"{directory}/suicide_watch.csv\")\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnDlcA7Blk4a"
   },
   "source": [
    "---\n",
    "\n",
    "# Merging data twitter-reddit\n",
    "\n",
    "- Standardise twitter columns\n",
    "  - Drop excess columns\n",
    "  - Followed reddit date format\n",
    "- Standardise reddit columns to twitter columns\n",
    "  - Remove invalid posts and users ~ Users and posts may be deleted or removed\n",
    "  - Rename columns\n",
    "  - Merge title and body (reddit posts) into body ~ twitter posts don't have titles\n",
    "  - Drop excess columns\n",
    "- Assign sentiment score based on subreddit pulled from\n",
    "- Merge into single dataframe\n",
    "\n",
    "*Assume posts from same subreddit have similar sentiment score; posts from [/r/depression](https://www.reddit.com/r/depression/), [/r/suicidewatch](https://www.reddit.com/r/SuicideWatch/) are negative.*\n",
    "\n",
    "*The size of twitter data from [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) far exceeds that of reddit data, so this assumption does not effect the data much.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NRkomAyHsn-s",
    "outputId": "6f329e31-367c-4010-ea5e-fa4397deed64"
   },
   "outputs": [],
   "source": [
    "def convert(date):\n",
    "\n",
    "    monthdict={\n",
    "        \"Jan\":\"01\", \n",
    "        \"Feb\":\"02\", \n",
    "        \"Mar\": \"03\", \n",
    "        \"Apr\":\"04\", \n",
    "        \"May\":\"05\", \n",
    "        \"Jun\":\"06\", \n",
    "        \"Jul\":\"07\", \n",
    "        \"Aug\":\"08\", \n",
    "        \"Sep\":\"09\", \n",
    "        \"Oct\":\"10\", \n",
    "        \"Nov\": \"11\", \n",
    "        \"Dec\":\"12\"}\n",
    "\n",
    "    year = date[24:28]\n",
    "    day = date[8:10]\n",
    "    month = date[4:7]\n",
    "    month = monthdict[month]\n",
    "    time= date[11:20]\n",
    "    unix= year +\"-\"+ month +\"-\"+ day + \" \" + time\n",
    "    \n",
    "    return unix\n",
    "\n",
    "def standardiseTwitterTweets(df):\n",
    "    \n",
    "    df = df.drop(columns=['query'])\n",
    "    df[\"date\"]=df[\"date\"].progress_map(convert)\n",
    "\n",
    "    return df\n",
    "\n",
    "df1 = standardiseTwitterTweets(df1)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "89TL01NjhuMs",
    "outputId": "945c2a84-67de-4fee-d23c-ec9c71e28087"
   },
   "outputs": [],
   "source": [
    "def removeInvalidRedditPost(df):\n",
    "    df = df[df.Body.notna()]\n",
    "    df = df[df.Author != \"[removed]\"]\n",
    "    df = df[df.Body != \"[removed]\"]\n",
    "    df = df[df.Author != \"[deleted]\"]\n",
    "    df = df[df.Body != \"[deleted]\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def standardiseRedditDF(dff, sentimentValue=None):\n",
    "\n",
    "    dff = removeInvalidRedditPost(dff)\n",
    "    dff.rename(columns={'Author': 'user_id', 'Post_iD': 'id', 'Publish_date':'date', 'Body':'text'}, inplace=True)\n",
    "    dff['text'] = dff['Title'].str.cat(dff['text'], sep=\" \")\n",
    "    dff = dff.drop(columns=['Score', 'Total_no_of_comments', 'Link', 'Subreddit', 'Title'])\n",
    "    \n",
    "    if sentimentValue != None:\n",
    "        dff['sentiment'] = sentimentValue\n",
    "\n",
    "    return dff\n",
    "\n",
    "df2 = standardiseRedditDF(df2, 0)\n",
    "df3 = standardiseRedditDF(df3, 0)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "A-vfLB6Llo8D",
    "outputId": "c052466e-6671-4d6e-e99a-a40813b0f025"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3])\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K65cIsYMsjg_"
   },
   "source": [
    "# Prepping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp8Ntbz7UK7a"
   },
   "source": [
    "## Mapping sentiments\n",
    "\n",
    "* 0 - negative\n",
    "* 2 - neutral\n",
    "* 4 - positive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhqboZgsUQs6",
    "outputId": "1205556e-a910-4794-a9c9-8d1a3dcf621a"
   },
   "outputs": [],
   "source": [
    "def sentimentMapping(label):\n",
    "    decodeMap = {0: \"Negative\", 2: \"Neutral\", 4: \"Positive\"}\n",
    "    return decodeMap[int(label)]\n",
    "\n",
    "df.sentiment = df.sentiment.progress_map(lambda x: sentimentMapping(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFq-yKEn6fuZ"
   },
   "source": [
    "## Cleaning text\n",
    "\n",
    "1. Lower casing\n",
    "2. Replacing URLs\n",
    "3. Replacing username references \n",
    "4. Removing non-alphanumerics\n",
    "5. Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQdcjKC26gQ_",
    "outputId": "39bbcb40-2299-4f0d-a0fc-27be6ad8eaa9"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = \"@[^\\s]+\" \n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "\n",
    "    text = sub(urlPattern, ' URL', text).strip()\n",
    "    text = sub(userPattern, ' USER', text).strip()\n",
    "    text = sub(alphaPattern, ' ', text).strip()\n",
    "    \n",
    "    stopWords = stopwords.words(\"english\")\n",
    "    tokens = list(filter(lambda x: x not in stopWords, text.split()))\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def parellelPreProcess(df):\n",
    "    ddf = dd.from_pandas(df, npartitions=4)\n",
    "    ddf[\"text\"] = ddf[\"text\"].map(lambda x: preprocess(x), meta=('result', str))\n",
    "    return ddf.compute()\n",
    "\n",
    "df = parellelPreProcess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned and merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "MYjcKvPmOWx4",
    "outputId": "2325a73e-d30a-42a1-92c5-d812b01f075d"
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloudPosNeg(df):\n",
    "    df_neg=df.loc[df['sentiment']==\"Negative\"]\n",
    "    df_pos=df.loc[df['sentiment']==\"Positive\"]\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update([\"a\", \"the\", \"I\", \"of\", \"then\", \"dont\", \"don\", \"\\m\",\"going\",\"don't\",\"make\",\"\\s\", \"m\", \"way\",\"day\",\"one\", \"s\", \"t\", \"dont\", \"ve\",\"USER\",\"URl\",\"amp\"])\n",
    "    \n",
    "    neg_text = \" \".join(str(review) for review in df_neg.text)\n",
    "    wordcloud_neg = WordCloud(stopwords=stopwords,max_font_size=300, max_words=100, width = 1200, height = 1200, scale = 1, background_color=\"white\").generate(neg_text)\n",
    "\n",
    "    pos_text = \" \".join(str(review) for review in df_pos.text)\n",
    "    wordcloud_pos = WordCloud(stopwords=stopwords,max_font_size=300, max_words=100, width = 1200, height = 1200, scale = 1, background_color=\"white\").generate(pos_text)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    axs[0].set_title(f\"Wordcloud negative sentiment\")\n",
    "    axs[0].axis('off')\n",
    "    axs[1].set_title(f\"Wordcloud positive sentiment\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[0].imshow(wordcloud_neg)\n",
    "    axs[1].imshow(wordcloud_pos)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "wordcloudPosNeg(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "## Generate model\n",
    "\n",
    "- Train and test splitting ~ 80/20 split\n",
    "- Tokenisation\n",
    "- Encoder\n",
    "- Building Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateModel(df):\n",
    "    \n",
    "    # Splitting train and test\n",
    "    trainData, testData = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    # Tokenisation\n",
    "    tokeniser = Tokenizer()\n",
    "    tokeniser.fit_on_texts(trainData.text)\n",
    "\n",
    "    vocabSize = len(tokeniser.word_index) + 1\n",
    "\n",
    "    # Encoder\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(trainData.sentiment.to_list())\n",
    "\n",
    "    # X and Y train and test\n",
    "    xTrain = pad_sequences(tokeniser.texts_to_sequences(trainData.text), maxlen = 300)\n",
    "    xTest = pad_sequences(tokeniser.texts_to_sequences(testData.text), maxlen = 300)\n",
    "\n",
    "    yTrain = encoder.transform(trainData.sentiment.to_list()).reshape(-1,1)\n",
    "    yTest = encoder.transform(testData.sentiment.to_list()).reshape(-1,1)\n",
    "\n",
    "    # Building Word2Vec and Embedding layer\n",
    "    w2vModel = Word2Vec(vector_size=300, window=7, min_count=10, workers=6)\n",
    "\n",
    "    _words = [_text.split() for _text in tqdm(trainData.text)]\n",
    "\n",
    "    w2vModel.build_vocab(_words)\n",
    "    w2vModel.train(tqdm(_words), \n",
    "                    total_examples=len(_words), \n",
    "                    epochs=8,\n",
    "                )\n",
    "\n",
    "    embMatrix = np.zeros((vocabSize, 300))\n",
    "    for word, i in tqdm(tokeniser.word_index.items()):\n",
    "        if word in w2vModel.wv:\n",
    "            embMatrix[i] = w2vModel.wv[word]\n",
    "\n",
    "    embLayer = Embedding(vocabSize, 300, weights=[embMatrix], input_length=300, trainable=False)\n",
    "\n",
    "    # Model building\n",
    "    model = Sequential()\n",
    "    model.add(embLayer)\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(CuDNNLSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return xTrain, yTrain, xTest, yTest, model, tokeniser\n",
    "\n",
    "xTrain, yTrain, xTest, yTest, model, tokeniser = generateModel(df)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wIIokKM2G1L"
   },
   "source": [
    "## Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5k6vR2zIGKqR",
    "outputId": "0359efb2-c853-4c51-aa1e-bd498c1015ae"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.fit(xTrain, yTrain,\n",
    "          batch_size=1024,\n",
    "          epochs=8,\n",
    "          validation_split=0.1,\n",
    "          verbose=0,\n",
    "          callbacks=[ \n",
    "            ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0), \n",
    "            EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5),\n",
    "            TqdmCallback(verbose=2)\n",
    "          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading model\n",
    "\n",
    "This function is to save a trained model and load a pre-trained model from model.h5. To use place 'model.h5' file within\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pc_aEQH0596"
   },
   "outputs": [],
   "source": [
    "def saveModel(model, tokeniser):\n",
    "    try:\n",
    "        model.save(\"model.h5\")\n",
    "        pickle.dump(tokeniser, open(\"tokenizer.pkl\", \"wb\"), protocol=0)\n",
    "        return \"Successfully saved\"\n",
    "    except:\n",
    "        return \"Failed save\"\n",
    "\n",
    "def loadModel(pathToModel, pathToPKL):\n",
    "    with open(pathToPKL, 'rb') as f:\n",
    "        tokeniser = pickle.load(f)\n",
    "    return load_model(pathToModel), tokeniser\n",
    "\n",
    "# Example usage\n",
    "# model, tokeniser = loadModel(\"./model.h5\", \"./tokenizer.pkl\")\n",
    "saveModel(model, tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqdBib_zHHc7"
   },
   "outputs": [],
   "source": [
    "def sentimentFromScore(score):\n",
    "  score = float(score)\n",
    "  label = 'Neutral'\n",
    "  if score <= 0.35:\n",
    "      label = 'Negative'\n",
    "  elif score >= 0.65:\n",
    "      label = 'Positive'\n",
    "\n",
    "  return label\n",
    "\n",
    "def predict(text, wantsTime=False):\n",
    "  if wantsTime: \n",
    "      start_at = time()\n",
    "  \n",
    "  text = str(text)\n",
    "  score = model.predict(pad_sequences(tokeniser.texts_to_sequences([text]), maxlen=300))\n",
    "\n",
    "  result = {\"label\": sentimentFromScore(score), \n",
    "            \"score\": score}\n",
    "  \n",
    "  if wantsTime: \n",
    "      result[\"elapsedTime\"] = time() - start_at\n",
    "\n",
    "  return result\n",
    "\n",
    "# Example usage\n",
    "prediction = predict(\"I'm sick of this game\", True)\n",
    "print(f\"Label: {prediction['label']}\")\n",
    "print(f\"Score: {prediction['score']}\")\n",
    "print(f\"Time elapsed: {prediction['elapsedTime']}s\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
